{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:07.259463Z",
     "iopub.status.busy": "2025-02-17T15:46:07.259068Z",
     "iopub.status.idle": "2025-02-17T15:46:07.638197Z",
     "shell.execute_reply": "2025-02-17T15:46:07.637286Z",
     "shell.execute_reply.started": "2025-02-17T15:46:07.259435Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "\n",
    "train_set = pd.read_csv('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv').loc[0,'text']\n",
    "valid_set = pd.read_csv('/kaggle/input/the-bards-best-a-character-modeling-dataset/validation.csv').loc[0,'text']\n",
    "test_set = pd.read_csv('/kaggle/input/the-bards-best-a-character-modeling-dataset/test.csv').loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:07.639641Z",
     "iopub.status.busy": "2025-02-17T15:46:07.639327Z",
     "iopub.status.idle": "2025-02-17T15:46:07.645378Z",
     "shell.execute_reply": "2025-02-17T15:46:07.644423Z",
     "shell.execute_reply.started": "2025-02-17T15:46:07.639616Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,text):\n",
    "        self.vocab = list(set(self.tokenize(text)))\n",
    "        self.encod_dict = { item:index  for index,item in enumerate(self.vocab)  }        \n",
    "        self.decode_dict = { index:item  for index,item in enumerate(self.vocab)  }\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    # def tokenize(self,text):\n",
    "    #     t = text.split(' ')\n",
    "    #     res = []\n",
    "    #     for item in t:\n",
    "    #         res.append(item)\n",
    "    #         res.append(' ')\n",
    "    #     return res\n",
    "    def tokenize(self, text):\n",
    "        return [text[i:i+1] for i in range(len(text))]\n",
    "\n",
    "    def encode(self,text):\n",
    "        return [self.encod_dict[item] for item in self.tokenize(text)]\n",
    "    def decode(self,tokens):\n",
    "        return ''.join([self.decode_dict[item] for item in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:07.647736Z",
     "iopub.status.busy": "2025-02-17T15:46:07.647411Z",
     "iopub.status.idle": "2025-02-17T15:46:10.800883Z",
     "shell.execute_reply": "2025-02-17T15:46:10.799979Z",
     "shell.execute_reply.started": "2025-02-17T15:46:07.647704Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "class DataShak(Dataset):\n",
    "    def __init__(self,tokenizer,text,context_size):\n",
    "        self.text = tokenizer.encode(text)\n",
    "        self.context_size = context_size\n",
    "    def __len__(self):\n",
    "        return len(self.text) // self.context_size\n",
    "    def __getitem__(self,idx):\n",
    "        next_idx = idx+1\n",
    "        x = self.text[idx*self.context_size:next_idx*self.context_size]\n",
    "        y = self.text[idx*self.context_size+1:next_idx*self.context_size+1]\n",
    "        if len(y) < self.context_size:\n",
    "            y = self.text[-self.context_size+1:] + [' ']\n",
    "        if len(x) < self.context_size:\n",
    "            x = self.text[-self.context_size:]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.802806Z",
     "iopub.status.busy": "2025-02-17T15:46:10.802253Z",
     "iopub.status.idle": "2025-02-17T15:46:10.807147Z",
     "shell.execute_reply": "2025-02-17T15:46:10.806113Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.802771Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x,y in batch:\n",
    "        xs.append(torch.tensor(x))\n",
    "        ys.append(torch.tensor(y))\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.808457Z",
     "iopub.status.busy": "2025-02-17T15:46:10.808138Z",
     "iopub.status.idle": "2025-02-17T15:46:10.824412Z",
     "shell.execute_reply": "2025-02-17T15:46:10.823634Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.808430Z"
    }
   },
   "outputs": [],
   "source": [
    "# class SimpleAverage(torch.nn.Module):\n",
    "#     def __init__(self,embed_dim,head_dim):\n",
    "#         super().__init__()\n",
    "#         self.value_projection = torch.nn.Sequential(torch.nn.Linear(embed_dim,head_dim))\n",
    "#         # self.key_projection = torch.nn.Sequential(torch.nn.Linear(embed_dim,head_dim))\n",
    "#         # self.queries_projection = torch.nn.Sequential(torch.nn.Linear(embed_dim,head_dim))\n",
    "#     def forward(self,x):\n",
    "#         tria = torch.tril(torch.ones(x.size(1),x.size(1)))\n",
    "#         avg_matrix = torch.zeros(x.size(1),x.size(1))\n",
    "#         avg_matrix = torch.nn.functional.softmax(avg_matrix.masked_fill(tria == 0,float('-inf')),-1).to(DEVICE)\n",
    "\n",
    "#         value = self.value_projection(x)\n",
    "#         mixed_value = avg_matrix @ value \n",
    "        \n",
    "#         return mixed_value\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self,embed_dim,head_dim):\n",
    "        super().__init__()\n",
    "        self.value_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "        self.key_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "        self.queries_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "        self.norm = head_dim**0.5\n",
    "    def forward(self,x):\n",
    "        tria = torch.tril(torch.ones(x.size(1),x.size(1))).to(DEVICE)\n",
    "        queries = self.queries_projection(x)\n",
    "        keys = self.key_projection(x)\n",
    "        att_matrix = queries @ keys.transpose(-2,-1) / self.norm\n",
    "        att_matrix = torch.nn.functional.softmax(att_matrix.masked_fill(tria == 0,float('-inf')),-1)\n",
    "\n",
    "        value = self.value_projection(x)\n",
    "        mixed_value = att_matrix @ value\n",
    "        \n",
    "        return mixed_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.825411Z",
     "iopub.status.busy": "2025-02-17T15:46:10.825192Z",
     "iopub.status.idle": "2025-02-17T15:46:10.838721Z",
     "shell.execute_reply": "2025-02-17T15:46:10.837869Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.825391Z"
    }
   },
   "outputs": [],
   "source": [
    "# class MultiHeadAverage(torch.nn.Module):\n",
    "#     def __init__(self,embed_dim,head_dim,num_heads):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.multi_heads = torch.nn.ModuleList([ SimpleAverage(embed_dim, head_dim // num_heads) for _ in range(num_heads)])\n",
    "#     def forward(self,x):\n",
    "#         xs = [ self.multi_heads[i](x) for i in range(self.num_heads) ]\n",
    "#         xs = torch.cat(xs,-1)\n",
    "#         return xs\n",
    "# class MultiHeadAttention(torch.nn.Module):\n",
    "#     def __init__(self,embed_dim,head_dim,num_heads):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.multi_heads = torch.nn.ModuleList([ CausalSelfAttention(embed_dim, head_dim // num_heads) for _ in range(num_heads)])\n",
    "#     def forward(self,x):\n",
    "#         xs = [ self.multi_heads[i](x) for i in range(self.num_heads) ]\n",
    "#         xs = torch.cat(xs,-1)\n",
    "#         return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.839801Z",
     "iopub.status.busy": "2025-02-17T15:46:10.839498Z",
     "iopub.status.idle": "2025-02-17T15:46:10.853629Z",
     "shell.execute_reply": "2025-02-17T15:46:10.852811Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.839778Z"
    }
   },
   "outputs": [],
   "source": [
    "# class MultiHeadedCausalSelfAttention(torch.nn.Module):\n",
    "#     def __init__(self,embed_dim,head_dim,num_heads):\n",
    "#         super().__init__()\n",
    "#         assert head_dim % num_heads == 0\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = head_dim\n",
    "#         self.value_projection = torch.nn.Linear(embed_dim,head_dim) \n",
    "#         self.key_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "#         self.queries_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "#         self.norm = (head_dim//num_heads)**0.5\n",
    "#     def forward(self,x):\n",
    "#         tria = torch.tril(torch.ones(x.size(1),x.size(1))).to(DEVICE)\n",
    "#         queries = self.queries_projection(x)  # ( B , C , D * N) N = Number of heads , C = Context Length , D = Head Embed Dim\n",
    "#         keys = self.key_projection(x) # ( B , C , D * N)\n",
    "#         value = self.value_projection(x) # ( B , C , D * N)\n",
    "\n",
    "#         queries = queries.reshape(x.size(0),x.size(1),self.num_heads,self.head_dim // self.num_heads).transpose(-3,-2) # ( B , N, C , D )\n",
    "#         keys = keys.reshape(x.size(0),x.size(1),self.num_heads,self.head_dim // self.num_heads).transpose(-3,-2) # ( B , N, C , D )\n",
    "#         value = value.reshape(x.size(0),x.size(1),self.num_heads,self.head_dim // self.num_heads).transpose(-3,-2) # ( B , N, C , D )\n",
    "        \n",
    "#         att_matrix = queries @ keys.transpose(-2,-1) / self.norm # ( B , N, C , C )\n",
    "#         att_matrix = torch.nn.functional.softmax(att_matrix.masked_fill(tria == 0,float('-inf')),-1)\n",
    "\n",
    "#         mixed_value = att_matrix @ value # ( B , N, C , D )\n",
    "#         mixed_value = mixed_value.transpose(-3,-2).reshape((x.size(0),x.size(1),self.head_dim))\n",
    "        \n",
    "#         return mixed_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.856562Z",
     "iopub.status.busy": "2025-02-17T15:46:10.856309Z",
     "iopub.status.idle": "2025-02-17T15:46:10.870099Z",
     "shell.execute_reply": "2025-02-17T15:46:10.869345Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.856532Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadedCausalSelfFlashAttention(torch.nn.Module):\n",
    "    def __init__(self,embed_dim,head_dim,num_heads):\n",
    "        super().__init__()\n",
    "        assert head_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.value_projection = torch.nn.Linear(embed_dim,head_dim) \n",
    "        self.key_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "        self.queries_projection = torch.nn.Linear(embed_dim,head_dim)\n",
    "        self.norm = (head_dim//num_heads)**0.5\n",
    "    def forward(self,x):\n",
    "        queries = self.queries_projection(x)  # ( B , C , D * N) N = Number of heads , C = Context Length , D = Head Embed Dim\n",
    "        keys = self.key_projection(x) # ( B , C , D * N)\n",
    "        value = self.value_projection(x) # ( B , C , D * N)\n",
    "\n",
    "        queries = queries.reshape(x.size(0),x.size(1),self.num_heads,self.head_dim // self.num_heads).transpose(-3,-2) # ( B , N, C , D )\n",
    "        keys = keys.reshape(x.size(0),x.size(1),self.num_heads,self.head_dim // self.num_heads).transpose(-3,-2) # ( B , N, C , D )\n",
    "        value = value.reshape(x.size(0),x.size(1),self.num_heads,self.head_dim // self.num_heads).transpose(-3,-2) # ( B , N, C , D )\n",
    "        \n",
    "\n",
    "        mixed_value = torch.nn.functional.scaled_dot_product_attention(queries,keys,value,is_causal=True) # ( B , N, C , D )\n",
    "        mixed_value = mixed_value.transpose(-3,-2).reshape((x.size(0),x.size(1),self.head_dim))\n",
    "        \n",
    "        return mixed_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.871686Z",
     "iopub.status.busy": "2025-02-17T15:46:10.871447Z",
     "iopub.status.idle": "2025-02-17T15:46:10.887039Z",
     "shell.execute_reply": "2025-02-17T15:46:10.886255Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.871667Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTMLP(torch.nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.ln_1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim,4*embed_dim)\n",
    "        self.actv = torch.nn.GELU()\n",
    "        self.proj = torch.nn.Linear(4*embed_dim,embed_dim)\n",
    "    def forward(self,x):\n",
    "        x = self.actv(self.fc(self.ln_1(x)))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.888198Z",
     "iopub.status.busy": "2025-02-17T15:46:10.887956Z",
     "iopub.status.idle": "2025-02-17T15:46:10.899281Z",
     "shell.execute_reply": "2025-02-17T15:46:10.898392Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.888173Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self,embed_dim,head_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadedCausalSelfFlashAttention(embed_dim,head_dim,num_heads)\n",
    "        self.proj = torch.nn.Linear(head_dim,embed_dim)\n",
    "        self.mlp = GPTMLP(embed_dim)\n",
    "    def forward(self,x):\n",
    "        x = x + self.proj(self.attn(self.ln_1(x)))\n",
    "        x = x + self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.900376Z",
     "iopub.status.busy": "2025-02-17T15:46:10.900124Z",
     "iopub.status.idle": "2025-02-17T15:46:10.915068Z",
     "shell.execute_reply": "2025-02-17T15:46:10.914271Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.900345Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.916179Z",
     "iopub.status.busy": "2025-02-17T15:46:10.915921Z",
     "iopub.status.idle": "2025-02-17T15:46:10.926271Z",
     "shell.execute_reply": "2025-02-17T15:46:10.925413Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.916150Z"
    }
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.927901Z",
     "iopub.status.busy": "2025-02-17T15:46:10.927677Z",
     "iopub.status.idle": "2025-02-17T15:46:10.939531Z",
     "shell.execute_reply": "2025-02-17T15:46:10.938708Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.927882Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self,vocab_size,context_size,embed_dim,head_dim,n_heads,n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wte = torch.nn.Embedding(vocab_size,embed_dim),\n",
    "            wpe = torch.nn.Embedding(context_size,embed_dim),\n",
    "            drop = torch.nn.Dropout(0.1),\n",
    "            h = torch.nn.ModuleList(\n",
    "                [TransformerBlock(embed_dim,head_dim,n_heads) for _ in range(n_layers)]\n",
    "            )\n",
    "        ))\n",
    "\n",
    "        self.lm_head = torch.nn.Linear(embed_dim,vocab_size)\n",
    "        self.register_buffer('wpe_sequence',torch.arange(0,context_size))\n",
    "\n",
    "        self.lm_head.weight = self.transformer.wte.weight\n",
    "    def forward(self,x):\n",
    "        x = self.transformer.wte(x)\n",
    "        x = x + self.transformer.wpe(self.wpe_sequence[:x.size(1)])\n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "        # Thinking Phase\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        #Output\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.940772Z",
     "iopub.status.busy": "2025-02-17T15:46:10.940457Z",
     "iopub.status.idle": "2025-02-17T15:46:10.977153Z",
     "shell.execute_reply": "2025-02-17T15:46:10.976506Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.940738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORLD_SIZE = torch.cuda.device_count()\n",
    "WORLD_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.978086Z",
     "iopub.status.busy": "2025-02-17T15:46:10.977858Z",
     "iopub.status.idle": "2025-02-17T15:46:10.981205Z",
     "shell.execute_reply": "2025-02-17T15:46:10.980270Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.978063Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONTEXT_SIZE = 256\n",
    "# EMBED_DIM = 128\n",
    "# NUM_HEADS = 4\n",
    "# NUM_LAYERS = 1\n",
    "# BATCH_SIZE = 32\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.982303Z",
     "iopub.status.busy": "2025-02-17T15:46:10.982101Z",
     "iopub.status.idle": "2025-02-17T15:46:10.993907Z",
     "shell.execute_reply": "2025-02-17T15:46:10.993164Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.982286Z"
    }
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 256\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "BATCH_SIZE = 700\n",
    "GRAD_ACCUMULATION_BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 1500\n",
    "LR = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:10.994811Z",
     "iopub.status.busy": "2025-02-17T15:46:10.994538Z",
     "iopub.status.idle": "2025-02-17T15:46:11.009449Z",
     "shell.execute_reply": "2025-02-17T15:46:11.008652Z",
     "shell.execute_reply.started": "2025-02-17T15:46:10.994791Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONTEXT_SIZE = 512\n",
    "# EMBED_DIM = 640\n",
    "# NUM_HEADS = 16\n",
    "# NUM_LAYERS = 6\n",
    "# BATCH_SIZE = 128\n",
    "# NUM_EPOCHS = 700\n",
    "# LR = 0.0005\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.010367Z",
     "iopub.status.busy": "2025-02-17T15:46:11.010122Z",
     "iopub.status.idle": "2025-02-17T15:46:11.020426Z",
     "shell.execute_reply": "2025-02-17T15:46:11.019648Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.010348Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONTEXT_SIZE = 256\n",
    "# EMBED_DIM = 256\n",
    "# NUM_HEADS = 8\n",
    "# NUM_LAYERS = 4\n",
    "# BATCH_SIZE = 64\n",
    "# NUM_EPOCHS = 10\n",
    "# LR = 0.005\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.021668Z",
     "iopub.status.busy": "2025-02-17T15:46:11.021353Z",
     "iopub.status.idle": "2025-02-17T15:46:11.032663Z",
     "shell.execute_reply": "2025-02-17T15:46:11.031706Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.021636Z"
    }
   },
   "outputs": [],
   "source": [
    "# x,y = next(iter(train_loader))\n",
    "# x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.033903Z",
     "iopub.status.busy": "2025-02-17T15:46:11.033580Z",
     "iopub.status.idle": "2025-02-17T15:46:11.044324Z",
     "shell.execute_reply": "2025-02-17T15:46:11.043518Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.033864Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, train_loader,optim,loss_fn,device):\n",
    "    model.train()\n",
    "\n",
    "    grad_acc_steps = GRAD_ACCUMULATION_BATCH_SIZE / BATCH_SIZE\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        total_loss = 0\n",
    "        token_th = 0\n",
    "        for batch_idx, (train_x, train_y) in enumerate(train_loader):\n",
    "            train_x = train_x.to(device)  # (B,T)\n",
    "            train_y = train_y.to(device)  # (B,T)\n",
    "    \n",
    "            pred_x = model(train_x)  # (B,T,VOCAB_SIZE)\n",
    "            loss = loss_fn(pred_x.view(-1, pred_x.size(-1)), train_y.view(-1))   # Flatten for CE loss\n",
    "            total_loss += loss.item() \n",
    "            token_th = token_th + train_x.size(0) * train_x.size(1)\n",
    "\n",
    "            loss = loss / grad_acc_steps \n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Accumulation\n",
    "            if (batch_idx) % grad_acc_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "                optim.step()\n",
    "                optim.zero_grad()  # Clear previous gradients\n",
    "            # if batch_idx % 10 == 0:\n",
    "            #     print(f\"Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        token_th = token_th / execution_time\n",
    "        print(f\"{device} Epoch {e} completed. Average Loss: {avg_loss:.4f} | Execution Time: {execution_time: .5f} s | Tokens Throughput : {token_th: .5f} t/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.045537Z",
     "iopub.status.busy": "2025-02-17T15:46:11.045236Z",
     "iopub.status.idle": "2025-02-17T15:46:11.059020Z",
     "shell.execute_reply": "2025-02-17T15:46:11.058253Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.045516Z"
    }
   },
   "outputs": [],
   "source": [
    "class AutoRegressiveGenerator():\n",
    "    def __init__(self,tokenizer,model,context_size,device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.context_size = context_size\n",
    "        self.device=device\n",
    "    def generate(self,inp,max_tokens,temperature,top_k=20):\n",
    "        encoded = torch.tensor(self.tokenizer.encode(inp),device=self.device).view(1,-1) # (B,T)\n",
    "        x = encoded\n",
    "        result = x\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_tokens):\n",
    "                x = x[:,-self.context_size:]\n",
    "                next_token_distribution =  torch.nn.functional.softmax(self.model(x)[:,-1,:] / temperature,-1)\n",
    "                # next_token = next_token_distribution.argmax(1).view(-1,1) # (B,1)\n",
    "                next_token = torch.multinomial(torch.topk(next_token_distribution,top_k,dim=1).values,1)\n",
    "                x = torch.cat((x,next_token),1) # (B,T+1)\n",
    "                result = torch.cat((result,next_token),1) # (B,T+1)\n",
    "            return self.tokenizer.decode(result.tolist()[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.060011Z",
     "iopub.status.busy": "2025-02-17T15:46:11.059789Z",
     "iopub.status.idle": "2025-02-17T15:46:11.074199Z",
     "shell.execute_reply": "2025-02-17T15:46:11.073288Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.059981Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.distributed import init_process_group,destroy_process_group\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.multiprocessing as mp\n",
    "import os\n",
    "def ddp_setup(rank,world_size):\n",
    "    os.environ['MASTER_ADDR']='localhost'\n",
    "    os.environ['MASTER_PORT']='12355'\n",
    "    init_process_group(backend='nccl',rank=rank,world_size=world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.077694Z",
     "iopub.status.busy": "2025-02-17T15:46:11.077414Z",
     "iopub.status.idle": "2025-02-17T15:46:11.087914Z",
     "shell.execute_reply": "2025-02-17T15:46:11.086998Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.077672Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "def main(rank,world_size,epoch):\n",
    "    try:\n",
    "        ddp_setup(rank,world_size)\n",
    "        DEVICE = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # model = torch.compile(model)\n",
    "        tokenizer = Tokenizer(train_set)\n",
    "        train_data = DataShak(tokenizer,train_set,CONTEXT_SIZE)\n",
    "        train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,collate_fn=collate_fn,shuffle=False,sampler=DistributedSampler(train_data))\n",
    "        loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "        model = GPT(tokenizer.vocab_size(),CONTEXT_SIZE,EMBED_DIM,EMBED_DIM,NUM_HEADS,NUM_LAYERS).to(DEVICE)\n",
    "        optim = torch.optim.Adam(model.parameters(),lr=LR)  # Instantiate optimizer with model parameters\n",
    "\n",
    "        model = DistributedDataParallel(model)\n",
    "        train(model,epoch,train_loader,optim,loss_fn,DEVICE)\n",
    "        gen = AutoRegressiveGenerator(tokenizer,model,CONTEXT_SIZE,DEVICE)\n",
    "        return gen.generate('As shall with either part',200,0.05)\n",
    "    finally:\n",
    "        destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:46:11.089287Z",
     "iopub.status.busy": "2025-02-17T15:46:11.089003Z",
     "iopub.status.idle": "2025-02-17T15:46:23.474819Z",
     "shell.execute_reply": "2025-02-17T15:46:23.474101Z",
     "shell.execute_reply.started": "2025-02-17T15:46:11.089257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 Epoch 0 completed. Average Loss: 199.0214 | Execution Time:  8.17040 s | Tokens Throughput :  122855.15501 t/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As shall with either partzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T15:50:06.069315Z",
     "iopub.status.busy": "2025-02-17T15:50:06.069005Z",
     "iopub.status.idle": "2025-02-17T15:50:08.078665Z",
     "shell.execute_reply": "2025-02-17T15:50:08.077398Z",
     "shell.execute_reply.started": "2025-02-17T15:50:06.069291Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 15:50:08.054000 31 torch/multiprocessing/spawn.py:160] Terminating process 88 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-eed30191d6d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWORLD_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWORLD_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[1;32m    327\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 )\n\u001b[1;32m    191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 raise ProcessExitedException(\n\u001b[0m\u001b[1;32m    193\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "mp.spawn(main,args=[WORLD_SIZE,1],nprocs=WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2660745,
     "sourceId": 4558742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
